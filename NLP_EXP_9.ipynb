{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siddhesh1503/NLP/blob/main/NLP_EXP_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NER using the NLTK(English Language)"
      ],
      "metadata": {
        "id": "8pTxw772qtxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install and import NLTK\n",
        "!pip install -q nltk\n",
        "import nltk\n",
        "import re\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk, Tree\n",
        "from nltk.corpus import names\n",
        "\n",
        "# Download required NLTK datasets\n",
        "datasets = ['punkt', 'averaged_perceptron_tagger', 'maxent_ne_chunker', 'words', 'names']\n",
        "for dataset in datasets:\n",
        "    nltk.download(dataset)\n",
        "\n",
        "# Load first names from NLTK\n",
        "first_names = set(name.lower() for name in names.words())\n",
        "\n",
        "# Extract standard NLTK named entities\n",
        "def extract_named_entities(tree):\n",
        "    entities = []\n",
        "    for chunk in tree:\n",
        "        if isinstance(chunk, Tree):\n",
        "            entity_name = \" \".join([token for token, pos in chunk.leaves()])\n",
        "            entity_type = chunk.label()\n",
        "            entities.append((entity_name, entity_type))\n",
        "    return entities\n",
        "\n",
        "# Extract additional entities like TIME, DATE, PERCENTAGE, EVENT, GAME\n",
        "def extract_additional_entities(text):\n",
        "    additional_entities = []\n",
        "\n",
        "    # Time pattern\n",
        "    time_pattern = r'\\b(?:[01]?\\d|2[0-3]):[0-5]\\d(?:\\s?[APap][Mm])?\\b|\\b(?:[1-9]|1[0-2])\\s?[APap][Mm]\\b'\n",
        "    times = re.findall(time_pattern, text)\n",
        "    additional_entities.extend([(t, 'TIME') for t in times])\n",
        "\n",
        "    # Date pattern\n",
        "    date_pattern = r'\\b(?:\\d{4}-\\d{2}-\\d{2})\\b|\\b(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|' \\\n",
        "                   r'Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\\s+\\d{1,2}(?:,\\s*\\d{4})?\\b'\n",
        "    dates = re.findall(date_pattern, text)\n",
        "    additional_entities.extend([(d, 'DATE') for d in dates])\n",
        "\n",
        "    # Percentage pattern\n",
        "    percentage_pattern = r'\\b\\d+(?:\\.\\d+)?%\\b'\n",
        "    percentages = re.findall(percentage_pattern, text)\n",
        "    additional_entities.extend([(p, 'PERCENTAGE') for p in percentages])\n",
        "\n",
        "    # Event keywords\n",
        "    events_list = ['concert', 'festival', 'conference', 'meeting', 'wedding']\n",
        "    for event in events_list:\n",
        "        if re.search(r'\\b' + re.escape(event) + r'\\b', text, re.IGNORECASE):\n",
        "            additional_entities.append((event, 'EVENT'))\n",
        "\n",
        "    # Game keywords\n",
        "    games_list = ['cricket', 'football', 'chess', 'tennis', 'hockey']\n",
        "    for game in games_list:\n",
        "        if re.search(r'\\b' + re.escape(game) + r'\\b', text, re.IGNORECASE):\n",
        "            additional_entities.append((game, 'GAME'))\n",
        "\n",
        "    return additional_entities\n",
        "\n",
        "# Correct misclassified PERSON entities\n",
        "def correct_person_entities(entities):\n",
        "    corrected_entities = []\n",
        "    for name, label in entities:\n",
        "        if label == 'GPE' and name.lower() in first_names:\n",
        "            corrected_entities.append((name, 'PERSON'))\n",
        "        else:\n",
        "            corrected_entities.append((name, label))\n",
        "    return corrected_entities\n",
        "\n",
        "# Main NER function\n",
        "def perform_ner(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    ner_tree = ne_chunk(pos_tags)\n",
        "\n",
        "    entities = extract_named_entities(ner_tree)\n",
        "    additional_entities = extract_additional_entities(text)\n",
        "\n",
        "    all_entities = entities + additional_entities\n",
        "    all_entities = correct_person_entities(all_entities)\n",
        "\n",
        "    return all_entities\n",
        "\n",
        "# Execution\n",
        "if __name__ == \"__main__\":\n",
        "    text_en = input(\"Enter English text: \")\n",
        "    entities = perform_ner(text_en)\n",
        "\n",
        "    print(\"\\nüîπ Optimized NLTK NER Results:\")\n",
        "    if entities:\n",
        "        for entity_name, entity_label in entities:\n",
        "            print(f\"Entity: {entity_name}, Type: {entity_label}\")\n",
        "    else:\n",
        "        print(\"No entities found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2veigoTUBwW",
        "outputId": "a9f3ad88-d01e-4170-c06c-bf2fb7531a94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/names.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter English text: Alice will travel to Paris for a music festival on September 15 at 7 PM and will play chess there with 60% chance of rain.\n",
            "\n",
            "üîπ Optimized NLTK NER Results:\n",
            "Entity: Alice, Type: PERSON\n",
            "Entity: Paris, Type: GPE\n",
            "Entity: 7 PM, Type: TIME\n",
            "Entity: September 15, Type: DATE\n",
            "Entity: festival, Type: EVENT\n",
            "Entity: chess, Type: GAME\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NER using the Spacy(English Language)"
      ],
      "metadata": {
        "id": "K76cognZvVeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import re\n",
        "\n",
        "# Download the small English model if not already present\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# Load the model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to extract additional entities like events and games using regex\n",
        "def extract_additional_entities(text):\n",
        "    additional_entities = []\n",
        "\n",
        "    # Event keywords example\n",
        "    events_list = ['concert', 'festival', 'conference', 'meeting', 'wedding']\n",
        "    for event in events_list:\n",
        "        if re.search(r'\\b' + re.escape(event) + r'\\b', text, re.IGNORECASE):\n",
        "            additional_entities.append((event, 'EVENT'))\n",
        "\n",
        "    # Game keywords example\n",
        "    games_list = ['cricket', 'football', 'chess', 'tennis', 'hockey']\n",
        "    for game in games_list:\n",
        "        if re.search(r'\\b' + re.escape(game) + r'\\b', text, re.IGNORECASE):\n",
        "            additional_entities.append((game, 'GAME'))\n",
        "\n",
        "    return additional_entities\n",
        "\n",
        "# Main NER function using spaCy\n",
        "def perform_ner(text):\n",
        "    doc = nlp(text)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    additional_entities = extract_additional_entities(text)\n",
        "    return entities + additional_entities\n",
        "\n",
        "# Main execution block\n",
        "if __name__ == \"__main__\":\n",
        "    text_en = input(\"Enter English text: \")\n",
        "    entities = perform_ner(text_en)\n",
        "\n",
        "    print(\"\\nüîπ spaCy NER Results:\")\n",
        "    if entities:\n",
        "        for entity_text, entity_label in entities:\n",
        "            print(f\"Entity: {entity_text}, Type: {entity_label}\")\n",
        "    else:\n",
        "        print(\"No entities found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iEVnWAHqy2u",
        "outputId": "d8916e92-fb4c-427a-f31d-d6d37068df45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Enter English text: John will attend a cricket match in London on August 20 at 5 PM.\n",
            "\n",
            "üîπ spaCy NER Results:\n",
            "Entity: John, Type: PERSON\n",
            "Entity: London, Type: GPE\n",
            "Entity: August 20, Type: DATE\n",
            "Entity: 5 PM, Type: TIME\n",
            "Entity: cricket, Type: GAME\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NER using the Stanza(English Language)"
      ],
      "metadata": {
        "id": "j8McjnWjvaVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install stanza\n",
        "!pip install -q stanza\n",
        "\n",
        "import stanza\n",
        "import re\n",
        "import logging\n",
        "\n",
        "logging.getLogger('stanza').setLevel(logging.ERROR)\n",
        "\n",
        "# Download English model (only once)\n",
        "stanza.download('en')\n",
        "\n",
        "# Initialize the pipeline\n",
        "nlp = stanza.Pipeline(lang='en', processors='tokenize,ner')\n",
        "\n",
        "# Event/Game keywords\n",
        "EVENTS_LIST = ['concert', 'festival', 'conference', 'meeting', 'wedding']\n",
        "GAMES_LIST = ['cricket', 'football', 'chess', 'tennis', 'hockey']\n",
        "\n",
        "# Function to extract additional entities (event/game)\n",
        "def extract_additional_entities(text):\n",
        "    additional_entities = []\n",
        "\n",
        "    for event in EVENTS_LIST:\n",
        "        if re.search(r'\\b' + re.escape(event) + r'\\b', text, re.IGNORECASE):\n",
        "            additional_entities.append((event, 'EVENT'))\n",
        "\n",
        "    for game in GAMES_LIST:\n",
        "        if re.search(r'\\b' + re.escape(game) + r'\\b', text, re.IGNORECASE):\n",
        "            additional_entities.append((game, 'GAME'))\n",
        "\n",
        "    return additional_entities\n",
        "\n",
        "# Main NER function\n",
        "def perform_ner(text):\n",
        "    doc = nlp(text)\n",
        "    entities = []\n",
        "\n",
        "    # Stanza NER extraction\n",
        "    for sent in doc.sentences:\n",
        "        for ent in sent.ents:\n",
        "            # ent.text: entity text, ent.type: entity type\n",
        "            entities.append((ent.text, ent.type))\n",
        "\n",
        "    # Add event/game entities\n",
        "    additional_entities = extract_additional_entities(text)\n",
        "    all_entities = entities + additional_entities\n",
        "\n",
        "    return all_entities\n",
        "\n",
        "# Execution\n",
        "if __name__ == \"__main__\":\n",
        "    text_en = input(\"Enter English text: \")\n",
        "    entities = perform_ner(text_en)\n",
        "\n",
        "    print(\"\\nüîπ Stanza NER Results:\")\n",
        "    if entities:\n",
        "        for entity_name, entity_label in entities:\n",
        "            print(f\"Entity: {entity_name}, Type: {entity_label}\")\n",
        "    else:\n",
        "        print(\"No entities found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTkn2q2Xq05w",
        "outputId": "07d2a4f1-cfa4-449b-9d5e-89487bd7c79c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter English text: Michael will attend the football championship in New York on October 10 at 3 PM and participate in a chess tournament.\n",
            "\n",
            "üîπ Stanza NER Results:\n",
            "Entity: Michael, Type: PERSON\n",
            "Entity: New York, Type: GPE\n",
            "Entity: October 10, Type: DATE\n",
            "Entity: 3 PM, Type: TIME\n",
            "Entity: football, Type: GAME\n",
            "Entity: chess, Type: GAME\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NER using the Stanza(Regional Language)"
      ],
      "metadata": {
        "id": "wd0CeJqKvg8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "import re\n",
        "import logging\n",
        "\n",
        "logging.getLogger('stanza').setLevel(logging.ERROR)  # Only show errors\n",
        "\n",
        "# -------------------- Initialize Pipelines --------------------\n",
        "nlp_hi = stanza.Pipeline(lang='hi', processors='tokenize,pos,ner', verbose=False)\n",
        "nlp_mr = stanza.Pipeline(lang='mr', processors='tokenize,pos,ner', verbose=False)\n",
        "\n",
        "# -------------------- Event/Game Keywords --------------------\n",
        "EVENTS_LIST_HI = ['‡§∏‡§Æ‡§æ‡§∞‡•ã‡§π', '‡§Æ‡§π‡•ã‡§§‡•ç‡§∏‡§µ', '‡§ï‡•â‡§®‡•ç‡§´‡•ç‡§∞‡•á‡§Ç‡§∏', '‡§Æ‡•Ä‡§ü‡§ø‡§Ç‡§ó', '‡§µ‡§ø‡§µ‡§æ‡§π', '‡§ï‡•ç‡§∞‡§ø‡§ï‡•á‡§ü ‡§ü‡•Ç‡§∞‡•ç‡§®‡§æ‡§Æ‡•á‡§Ç‡§ü', '‡§∏‡§Ç‡§ó‡•Ä‡§§ ‡§Æ‡§π‡•ã‡§§‡•ç‡§∏‡§µ']\n",
        "GAMES_LIST_HI = ['‡§ï‡•ç‡§∞‡§ø‡§ï‡•á‡§ü', '‡§´‡•Å‡§ü‡§¨‡•â‡§≤', '‡§∂‡§§‡§∞‡§Ç‡§ú', '‡§ü‡•á‡§®‡§ø‡§∏', '‡§π‡•â‡§ï‡•Ä']\n",
        "EVENTS_LIST_MR = ['‡§∏‡§Æ‡§æ‡§∞‡§Ç‡§≠', '‡§â‡§§‡•ç‡§∏‡§µ', '‡§∏‡§Æ‡•ç‡§Æ‡•á‡§≤‡§®', '‡§Æ‡•Ä‡§ü‡§ø‡§Ç‡§ó', '‡§≤‡§ó‡•ç‡§®', '‡§∏‡§Ç‡§ó‡•Ä‡§§ ‡§â‡§§‡•ç‡§∏‡§µ', '‡§ï‡•ç‡§∞‡§ø‡§ï‡•á‡§ü ‡§∏‡•ç‡§™‡§∞‡•ç‡§ß‡§æ']\n",
        "GAMES_LIST_MR = ['‡§ï‡•ç‡§∞‡§ø‡§ï‡•á‡§ü', '‡§´‡•Å‡§ü‡§¨‡•â‡§≤', '‡§∂‡§§‡§∞‡§Ç‡§ú', '‡§ü‡•á‡§®‡§ø‡§∏', '‡§π‡•â‡§ï‡•Ä']\n",
        "\n",
        "# -------------------- Names Keywords --------------------\n",
        "NAMES_LIST_HI = ['‡§∞‡§æ‡§π‡•Å‡§≤', '‡§∏‡§æ‡§∞‡§æ', '‡§Ö‡§Æ‡§ø‡§§', '‡§™‡•Å‡§®‡•Ä‡§§']\n",
        "NAMES_LIST_MR = ['‡§∏‡§æ‡§∞‡§æ', '‡§∞‡•ã‡§π‡§ø‡§§', '‡§Ö‡§≠‡§ø‡§ú‡•Ä‡§§', '‡§®‡•á‡§π‡§æ']\n",
        "\n",
        "# -------------------- Regex Patterns --------------------\n",
        "TIME_PATTERN = r'\\b(?:‡§è‡§ï|‡§¶‡•ã|‡§§‡•Ä‡§®|‡§ö‡§æ‡§∞|‡§™‡§æ‡§Å‡§ö|‡§õ‡§π|‡§∏‡§æ‡§§|‡§Ü‡§†|‡§®‡•å|‡§¶‡§∏|‡§ó‡•ç‡§Ø‡§æ‡§∞‡§π|‡§¨‡§æ‡§∞‡§π|‡§ö‡§æ‡§∞|‡§∏‡§π‡§æ|‡§§‡•Ä‡§®)\\s?‡§¨‡§ú‡•á\\b|' \\\n",
        "               r'\\b(?:‡§è‡§ï|‡§¶‡•ã‡§®|‡§§‡•Ä‡§®|‡§ö‡§æ‡§∞|‡§™‡§æ‡§ö|‡§∏‡§π‡§æ|‡§∏‡§æ‡§§|‡§Ü‡§†|‡§®‡§ä|‡§¶‡§π‡§æ|‡§Ö‡§ï‡§∞‡§æ|‡§¨‡§æ‡§∞‡§æ|‡§∏‡§π‡§æ|‡§§‡•Ä‡§®)\\s?‡§µ‡§æ‡§ú‡§§‡§æ\\b'\n",
        "DATE_PATTERN_HI = r'\\b(?:‡§è‡§ï|‡§¶‡•ã|‡§§‡•Ä‡§®|‡§ö‡§æ‡§∞|‡§™‡§æ‡§Å‡§ö|‡§õ‡§π|‡§∏‡§æ‡§§|‡§Ü‡§†|‡§®‡•å|‡§¶‡§∏|‡§ó‡•ç‡§Ø‡§æ‡§∞‡§π|‡§¨‡§æ‡§∞‡§π|‡§§‡•á‡§∞‡§π|‡§ö‡•å‡§¶‡§π|‡§™‡§Ç‡§¶‡•ç‡§∞‡§π|‡§∏‡•ã‡§≤‡§π|‡§∏‡§§‡•ç‡§∞‡§π|‡§Ö‡§†‡§æ‡§∞‡§π|‡§â‡§®‡•ç‡§®‡•Ä‡§∏|‡§¨‡•Ä‡§∏|‡§á‡§ï‡•ç‡§ï‡•Ä‡§∏|‡§¨‡§æ‡§à‡§∏|‡§§‡•á‡§à‡§∏|‡§ö‡•å‡§¨‡•Ä‡§∏|‡§™‡§ö‡•ç‡§ö‡•Ä‡§∏|‡§õ‡§¨‡•ç‡§¨‡•Ä‡§∏|‡§∏‡§§‡•ç‡§§‡§æ‡§à‡§∏|‡§Ö‡§ü‡•ç‡§†‡§æ‡§à‡§∏|‡§â‡§®‡§§‡•Ä‡§∏|‡§§‡•Ä‡§∏)\\s?(?:‡§ú‡§®‡§µ‡§∞‡•Ä|‡§´‡§∞‡§µ‡§∞‡•Ä|‡§Æ‡§æ‡§∞‡•ç‡§ö|‡§Ö‡§™‡•ç‡§∞‡•à‡§≤|‡§Æ‡§à|‡§ú‡•Ç‡§®|‡§ú‡•Å‡§≤‡§æ‡§à|‡§Ö‡§ó‡§∏‡•ç‡§§|‡§∏‡§ø‡§§‡§Ç‡§¨‡§∞|‡§Ö‡§ï‡•ç‡§ü‡•Ç‡§¨‡§∞|‡§®‡§µ‡§Ç‡§¨‡§∞|‡§¶‡§ø‡§∏‡§Ç‡§¨‡§∞)\\b'\n",
        "DATE_PATTERN_MR = r'\\b(?:‡§è‡§ï|‡§¶‡•ã‡§®|‡§§‡•Ä‡§®|‡§ö‡§æ‡§∞|‡§™‡§æ‡§ö|‡§∏‡§π‡§æ|‡§∏‡§æ‡§§|‡§Ü‡§†|‡§®‡§ä|‡§¶‡§π‡§æ|‡§Ö‡§ï‡§∞‡§æ|‡§¨‡§æ‡§∞‡§æ|‡§§‡•á‡§∞‡§æ|‡§ö‡•å‡§¶‡§æ|‡§™‡§Ç‡§ß‡§∞‡§æ|‡§∏‡•ã‡§≤‡§æ|‡§∏‡§§‡§∞|‡§Ö‡§†‡§∞‡§æ|‡§è‡§ï‡•ã‡§£‡•Ä‡§∏|‡§µ‡•Ä‡§∏|‡§è‡§ï‡§µ‡•Ä‡§∏|‡§¨‡§æ‡§µ‡•Ä‡§∏|‡§§‡•á‡§µ‡•Ä‡§∏|‡§ö‡•ã‡§µ‡•Ä‡§∏|‡§™‡§Ç‡§ö‡§µ‡•Ä‡§∏|‡§∏‡§µ‡•ç‡§µ‡•Ä‡§∏|‡§∏‡§§‡•ç‡§§‡§æ‡§µ‡•Ä‡§∏|‡§Ö‡§†‡•ç‡§†‡§æ‡§µ‡•Ä‡§∏|‡§è‡§ï‡•ã‡§£‡§§‡•Ä‡§∏|‡§§‡•Ä‡§∏)\\s?(?:‡§ú‡§æ‡§®‡•á‡§µ‡§æ‡§∞‡•Ä|‡§´‡•á‡§¨‡•ç‡§∞‡•Å‡§µ‡§æ‡§∞‡•Ä|‡§Æ‡§æ‡§∞‡•ç‡§ö|‡§è‡§™‡•ç‡§∞‡§ø‡§≤|‡§Æ‡•á|‡§ú‡•Ç‡§®|‡§ú‡•Å‡§≤‡•à|‡§ë‡§ó‡§∏‡•ç‡§ü|‡§∏‡§™‡•ç‡§ü‡•á‡§Ç‡§¨‡§∞|‡§ë‡§ï‡•ç‡§ü‡•ã‡§¨‡§∞|‡§®‡•ã‡§µ‡•ç‡§π‡•á‡§Ç‡§¨‡§∞|‡§°‡§ø‡§∏‡•á‡§Ç‡§¨‡§∞)\\b'\n",
        "PERCENT_PATTERN = r'\\b(?:‡§∏‡§æ‡§†|‡§™‡§Ç‡§ö‡§æ‡§π‡§§‡•ç‡§§‡§∞|‡§∏‡§§‡•ç‡§§‡§∞|‡§ê‡§Ç‡§∂‡•Ä|‡§®‡§µ‡•ç‡§µ‡§¶|‡§™‡§ö‡§æ‡§∏)\\s?(?:‡§™‡•ç‡§∞‡§§‡§ø‡§∂‡§§|‡§ü‡§ï‡•ç‡§ï‡•á)\\b'\n",
        "\n",
        "# -------------------- Functions --------------------\n",
        "def extract_regex_entities(text, lang):\n",
        "    entities = []\n",
        "\n",
        "    # TIME\n",
        "    times = re.findall(TIME_PATTERN, text)\n",
        "    entities.extend([(t, 'TIME') for t in times])\n",
        "\n",
        "    # DATE\n",
        "    date_pattern = DATE_PATTERN_HI if lang == 'hi' else DATE_PATTERN_MR\n",
        "    dates = re.findall(date_pattern, text)\n",
        "    entities.extend([(d, 'DATE') for d in dates])\n",
        "\n",
        "    # PERCENTAGE\n",
        "    percentages = re.findall(PERCENT_PATTERN, text)\n",
        "    entities.extend([(p, 'PERCENTAGE') for p in percentages])\n",
        "\n",
        "    # EVENT / GAME keywords\n",
        "    events_list = EVENTS_LIST_HI if lang == 'hi' else EVENTS_LIST_MR\n",
        "    games_list = GAMES_LIST_HI if lang == 'hi' else GAMES_LIST_MR\n",
        "\n",
        "    for event in events_list:\n",
        "        if event in text:\n",
        "            entities.append((event, 'EVENT'))\n",
        "\n",
        "    for game in games_list:\n",
        "        if game in text:\n",
        "            entities.append((game, 'GAME'))\n",
        "\n",
        "    return entities\n",
        "\n",
        "def extract_names(text, lang):\n",
        "    names_list = NAMES_LIST_HI if lang == 'hi' else NAMES_LIST_MR\n",
        "    entities = []\n",
        "    for name in names_list:\n",
        "        if name in text:\n",
        "            entities.append((name, 'PERSON'))\n",
        "    return entities\n",
        "\n",
        "def process_sentence(text, nlp, lang):\n",
        "    doc = nlp(text)\n",
        "    entities = []\n",
        "\n",
        "    # PERSON & GPE from Stanza\n",
        "    for sent in doc.sentences:\n",
        "        for ent in sent.ents:\n",
        "            if ent.type in ['PERSON', 'LOC', 'GPE', 'NEL']:\n",
        "                entities.append((ent.text, 'PERSON' if ent.type=='PERSON' else 'GPE'))\n",
        "\n",
        "    # Add regex-based entities\n",
        "    entities += extract_regex_entities(text, lang)\n",
        "\n",
        "    # Add names from keyword list\n",
        "    entities += extract_names(text, lang)\n",
        "\n",
        "    # Remove duplicates\n",
        "    seen = set()\n",
        "    final_entities = []\n",
        "    for e in entities:\n",
        "        if e not in seen:\n",
        "            final_entities.append(e)\n",
        "            seen.add(e)\n",
        "\n",
        "    return final_entities\n",
        "\n",
        "# -------------------- New Sentences --------------------\n",
        "hindi_sentence = \"‡§∞‡§æ‡§π‡•Å‡§≤ ‡§Æ‡•Å‡§Ç‡§¨‡§à ‡§ï‡•á ‡§ï‡•ç‡§∞‡§ø‡§ï‡•á‡§ü ‡§ü‡•Ç‡§∞‡•ç‡§®‡§æ‡§Æ‡•á‡§Ç‡§ü ‡§Æ‡•á‡§Ç ‡§¨‡•Ä‡§∏ ‡§Ö‡§ó‡§∏‡•ç‡§§ ‡§ï‡•ã ‡§ö‡§æ‡§∞ ‡§¨‡§ú‡•á ‡§≠‡§æ‡§ó ‡§≤‡•á‡§Ç‡§ó‡•á ‡§î‡§∞ ‡§∂‡§§‡§∞‡§Ç‡§ú ‡§ñ‡•á‡§≤‡•á‡§Ç‡§ó‡•á, ‡§¨‡§æ‡§∞‡§ø‡§∂ ‡§ï‡•Ä ‡§™‡§ö‡§æ‡§∏ ‡§™‡•ç‡§∞‡§§‡§ø‡§∂‡§§ ‡§∏‡§Ç‡§≠‡§æ‡§µ‡§®‡§æ ‡§π‡•à‡•§\"\n",
        "marathi_sentence = \"‡§∏‡§æ‡§∞‡§æ ‡§™‡•Å‡§£‡•ç‡§Ø‡§æ‡§§‡•Ä‡§≤ ‡§∏‡§Ç‡§ó‡•Ä‡§§ ‡§â‡§§‡•ç‡§∏‡§µ‡§æ‡§§ ‡§™‡§Ç‡§ß‡§∞‡§æ ‡§∏‡§™‡•ç‡§ü‡•á‡§Ç‡§¨‡§∞ ‡§∞‡•ã‡§ú‡•Ä ‡§∏‡§π‡§æ ‡§µ‡§æ‡§ú‡§§‡§æ ‡§∏‡§π‡§≠‡§æ‡§ó‡•Ä ‡§π‡•ã‡§à‡§≤ ‡§Ü‡§£‡§ø ‡§ü‡•á‡§®‡§ø‡§∏ ‡§ñ‡•á‡§≥‡•á‡§≤, ‡§µ‡§ø‡§ú‡•á‡§§‡•á‡§™‡§¶‡§æ‡§ö‡•Ä ‡§∏‡§æ‡§† ‡§ü‡§ï‡•ç‡§ï‡•á ‡§∂‡§ï‡•ç‡§Ø‡§§‡§æ ‡§Ü‡§π‡•á‡•§\"\n",
        "\n",
        "# -------------------- Process Hindi --------------------\n",
        "entities_hi = process_sentence(hindi_sentence, nlp_hi, 'hi')\n",
        "print(\"\\nüîπ Stanza NER Result (Hindi):\")\n",
        "for text, label in entities_hi:\n",
        "    print(f\"Entity: {text}, Type: {label}\")\n",
        "\n",
        "# -------------------- Process Marathi --------------------\n",
        "entities_mr = process_sentence(marathi_sentence, nlp_mr, 'mr')\n",
        "print(\"\\nüîπ Stanza NER Result (Marathi):\")\n",
        "for text, label in entities_mr:\n",
        "    print(f\"Entity: {text}, Type: {label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6NUDfOKFbL8",
        "outputId": "c68d36ad-e8ca-4f80-cb0f-7c0e2b79f671"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîπ Stanza NER Result (Hindi):\n",
            "Entity: ‡§Æ‡•Å‡§Ç‡§¨‡§à, Type: GPE\n",
            "Entity: ‡§¨‡•Ä‡§∏ ‡§Ö‡§ó‡§∏‡•ç‡§§, Type: DATE\n",
            "Entity: ‡§™‡§ö‡§æ‡§∏ ‡§™‡•ç‡§∞‡§§‡§ø‡§∂‡§§, Type: PERCENTAGE\n",
            "Entity: ‡§ï‡•ç‡§∞‡§ø‡§ï‡•á‡§ü ‡§ü‡•Ç‡§∞‡•ç‡§®‡§æ‡§Æ‡•á‡§Ç‡§ü, Type: EVENT\n",
            "Entity: ‡§ï‡•ç‡§∞‡§ø‡§ï‡•á‡§ü, Type: GAME\n",
            "Entity: ‡§∂‡§§‡§∞‡§Ç‡§ú, Type: GAME\n",
            "Entity: ‡§∞‡§æ‡§π‡•Å‡§≤, Type: PERSON\n",
            "\n",
            "üîπ Stanza NER Result (Marathi):\n",
            "Entity: ‡§™‡•Å‡§£‡•ç‡§Ø‡§æ‡§§‡•Ä‡§≤, Type: GPE\n",
            "Entity: ‡§™‡§Ç‡§ß‡§∞‡§æ ‡§∏‡§™‡•ç‡§ü‡•á‡§Ç‡§¨‡§∞, Type: DATE\n",
            "Entity: ‡§â‡§§‡•ç‡§∏‡§µ, Type: EVENT\n",
            "Entity: ‡§∏‡§Ç‡§ó‡•Ä‡§§ ‡§â‡§§‡•ç‡§∏‡§µ, Type: EVENT\n",
            "Entity: ‡§ü‡•á‡§®‡§ø‡§∏, Type: GAME\n",
            "Entity: ‡§∏‡§æ‡§∞‡§æ, Type: PERSON\n"
          ]
        }
      ]
    }
  ]
}